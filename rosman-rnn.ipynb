{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor,plot_importance\n",
    "from scipy.stats import f_oneway\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA1\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "sales = pd.read_csv('D:/승찬/수요 예측/rossmann-store-sales/train.csv')\n",
    "store = pd.read_csv('D:/승찬/수요 예측/rossmann-store-sales/store.csv')\n",
    "future_sales = pd.read_csv('D:/승찬/수요 예측/rossmann-store-sales/test.csv')\n",
    "sales['StateHoliday'] = sales['StateHoliday'].replace({0:\"0\"})\n",
    "sales['StateHoliday'] = sales['StateHoliday'].replace({'0':'d'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(sales,store) :\n",
    "    sales['Date'] = pd.to_datetime(sales['Date'], format=\"%Y-%m-%d\") # 년도, 달, 일 순으로 format\n",
    "    sales['StateHoliday'] = sales['StateHoliday'].replace({0:\"0\"})\n",
    "    sales['StateHoliday'] = sales['StateHoliday'].replace({'0':'d'})\n",
    "    sales = sales.sort_values('Date')\n",
    "    \n",
    "    one_hot = []\n",
    "    for i in range(0,len(sales)) :\n",
    "        one_hot.append(0)\n",
    "        \n",
    "    name = ['StateHoliday_a','StateHoliday_b','StateHoliday_c','StateHoliday_d']\n",
    "    values = ['a','b','c','d']\n",
    "    counts = sales['StateHoliday'].value_counts().sort_index().index\n",
    "\n",
    "    for i in range(0,len(values)) :\n",
    "        if values[i] not in counts :\n",
    "            sales[name[i]] = one_hot        \n",
    "    sales = pd.get_dummies(data = sales, columns = ['StateHoliday'])\n",
    "    columns = sales.columns.tolist()\n",
    "    columns.sort()\n",
    "    sales = sales[columns]\n",
    "    \n",
    "    store = processing_store(store)\n",
    "    store_copy = store\n",
    "    store_copy= pd.merge(left=sales,right=store_copy,on='Store')\n",
    "\n",
    "    store_copy['Year']=pd.DatetimeIndex(store_copy.Date).year\n",
    "    store_copy['Month']=pd.DatetimeIndex(store_copy.Date).month\n",
    "    store_copy['Day']=pd.DatetimeIndex(store_copy.Date).day\n",
    "    if 'Id' in sales :\n",
    "        store_copy = store_copy.drop(['Date','Store','Id'],axis=1)\n",
    "    else :\n",
    "        store_copy = store_copy.drop(['Date','Store','Customers'],axis=1)\n",
    "        \n",
    "    target_col = 'Sales'\n",
    "    input_cols = store_copy.columns.drop(target_col)        \n",
    "    features = store_copy[input_cols]\n",
    "    target = store_copy[target_col]\n",
    "    target = pd.DataFrame(target.values.reshape(-1,1))\n",
    "        \n",
    "#     StandardScaler\n",
    "#     scaler = StandardScaler().fit(store_copy)\n",
    "#     store_copy = pd.DataFrame(scaler.transform(store_copy),\n",
    "#                              index = store_copy.index, columns = store_copy.columns)\n",
    "    \n",
    "#    MinMaxScaler\n",
    "#     scaler = MinMaxScaler().fit(store_copy)\n",
    "#     store_copy = pd.DataFrame(scaler.transform(store_copy),\n",
    "#                              index = store_copy.index, columns = store_copy.columns)\n",
    "\n",
    "#     store_copy = feature_engineering_scaling(store_copy)\n",
    "            \n",
    "    return make_regression_model_1(features,target)\n",
    "\n",
    "def processing_store(store) :\n",
    "    store_copy = store.copy()\n",
    "    store_copy['CompetitionDistance'] = store_copy['CompetitionDistance'].fillna(\n",
    "        store_copy['CompetitionDistance'].mean())\n",
    "\n",
    "    store_copy_cols = ['CompetitionOpenSinceYear','CompetitionOpenSinceMonth',\n",
    "                    'Promo2SinceWeek','Promo2SinceYear','PromoInterval']\n",
    "    for i in store_copy_cols :\n",
    "        store_copy[i].fillna(0,inplace=True)\n",
    "    store_copy = pd.get_dummies(data=store_copy,columns=['Assortment','PromoInterval','StoreType'])\n",
    "    return store_copy   \n",
    "\n",
    "def scaling_data(scaler,features,target) :\n",
    "    global scaler2\n",
    "    if scaler == StandardScaler :\n",
    "        scaler1 = StandardScaler().fit(features)\n",
    "#         scaler2 = StandardScaler().fit(target)\n",
    "        features = pd.DataFrame(scaler1.transform(features),\n",
    "                               index = features.index, columns = features.columns)\n",
    "#         target = pd.DataFrame(scaler2.transform(target),\n",
    "#                              index = target.index, columns = target.columns)\n",
    "        target = np.log1p(target)\n",
    "    elif scaler == MinMaxScaler : \n",
    "        scaler1 = StandardScaler().fit(features)\n",
    "        scaler2 = MinMaxScaler().fit(target)\n",
    "        features = pd.DataFrame(scaler1.transform(features),\n",
    "                               index = features.index,columns = features.columns)\n",
    "        target = pd.DataFrame(scaler2.transform(target),\n",
    "                             index = target.index, columns = target.columns)\n",
    "    elif scaler == normal :\n",
    "        return features,target\n",
    "        \n",
    "    \n",
    "    return features,target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inverse_scaling(target) :\n",
    "    global scaler2\n",
    "    inverse_scaled_data = scaler2.inverse_transform(target)\n",
    "    \n",
    "    return inverse_scaled_data\n",
    "    \n",
    "    \n",
    "\n",
    "def make_regression_model_1(features,target) :\n",
    "    features, target = scaling_data(StandardScaler,features,target)\n",
    "    train_x,test_x,train_y,test_y = train_test_split(features,\n",
    "                                                    target,\n",
    "                                                    test_size=0.4,random_state=1)\n",
    "    \n",
    "    test_x,val_x,test_y,val_y = train_test_split(test_x,\n",
    "                                                test_y,\n",
    "                                                test_size=0.5)\n",
    "       \n",
    "    return train_x,test_x,val_x,train_y,test_y,val_y\n",
    "\n",
    "def type_name(feature,target,loss_function) :\n",
    "    data = [feature+'_'+target+'_'+loss_function+'_param',\n",
    "            feature+'_'+target+'_'+loss_function+'_predict',\n",
    "            feature+'_'+target+'_'+loss_function+'_hist']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,val_x,train_y,test_y,val_y=processing_data(sales,store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565, 28)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(sales,store) :\n",
    "    sales['Date'] = pd.to_datetime(sales['Date'], format=\"%Y-%m-%d\") # 년도, 달, 일 순으로 format\n",
    "    sales['StateHoliday'] = sales['StateHoliday'].replace({0:\"0\"})\n",
    "    sales['StateHoliday'] = sales['StateHoliday'].replace({'0':'d'})\n",
    "    sales = sales.sort_values('Date')\n",
    "\n",
    "    one_hot = []\n",
    "    for i in range(0,len(sales)) :\n",
    "        one_hot.append(0)\n",
    "\n",
    "    name = ['StateHoliday_a','StateHoliday_b','StateHoliday_c','StateHoliday_d']\n",
    "    values = ['a','b','c','d']\n",
    "    counts = sales['StateHoliday'].value_counts().sort_index().index\n",
    "\n",
    "    for i in range(0,len(values)) :\n",
    "        if values[i] not in counts :\n",
    "            sales[name[i]] = one_hot        \n",
    "    sales = pd.get_dummies(data = sales, columns = ['StateHoliday'])\n",
    "    columns = sales.columns.tolist()\n",
    "    columns.sort()\n",
    "    sales = sales[columns]\n",
    "\n",
    "    store = processing_store(store)\n",
    "    store_copy = store\n",
    "    test = (sales.Store == 1)\n",
    "    sales_copy = sales[test]\n",
    "    store_copy= pd.merge(left=sales_copy,right=store_copy,on='Store')\n",
    "    store_copy['Year']=pd.DatetimeIndex(store_copy.Date).year\n",
    "    store_copy['Month']=pd.DatetimeIndex(store_copy.Date).month\n",
    "    store_copy['Day']=pd.DatetimeIndex(store_copy.Date).day\n",
    "    if 'Id' in sales :\n",
    "        store_copy = store_copy.drop(['Date','Store','Id'],axis=1)\n",
    "    else :\n",
    "        store_copy = store_copy.drop(['Date','Store','Customers'],axis=1)\n",
    "\n",
    "    target_col = 'Sales'\n",
    "    input_cols = store_copy.columns.drop(target_col)        \n",
    "    features = store_copy[input_cols]\n",
    "    target = store_copy[target_col]\n",
    "    target = pd.DataFrame(target.values.reshape(-1,1))\n",
    "    features_copy = []\n",
    "    target_copy = []\n",
    "    for i in range(len(features)-windows_size) :\n",
    "        features_copy.append(np.array(features.iloc[i:i+windows_size]))\n",
    "        target_copy.append(np.array(target.iloc[i:i+windows_size]))\n",
    "\n",
    "    features,target = scaling_data(StandardScaler,features,target)\n",
    "\n",
    "    return make_timeseries_model(features,target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_copy = []\n",
    "target_copy = []\n",
    "for i in range(len(features)-windows_size) :\n",
    "    features_copy.append(np.array(features.iloc[i:i+windows_size]))\n",
    "    target_copy.append(np.array(target.iloc[i:i+windows_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912, 30, 28)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(features_copy).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_timeseries_model(features,target) :\n",
    "#     features, target = scaling_data(StandardScaler,features,target)\n",
    "    train_x,test_x,train_y,test_y = train_test_split(features,\n",
    "                                                    target,\n",
    "                                                    test_size=0.4,random_state=1)\n",
    "    \n",
    "    test_x,val_x,test_y,val_y = train_test_split(test_x,\n",
    "                                                test_y,\n",
    "                                                test_size=0.5)\n",
    "       \n",
    "    return train_x,test_x,val_x,train_y,test_y,val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,val_x,train_y,test_y,val_y = make_timeseries_model(features_copy,target_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-161-04aa4f6d450a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
